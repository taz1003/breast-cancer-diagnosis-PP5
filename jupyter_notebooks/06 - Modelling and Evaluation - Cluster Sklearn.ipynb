{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Cluster**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Fit and evaluate a cluster model to group similar data\n",
        "* Analyse the clusters against the diagnostic (malignant or benign)\n",
        "* Understand the profile for each cluster\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* Outputs/datasets/collection/breast-cancer.csv\n",
        "* Instructions on which variables to use for data cleaning and feature engineering, which are found in their respective notebooks.\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Cluster Pipeline\n",
        "* Train Set\n",
        "* Most important features to define a cluster plot\n",
        "* Clusters Profile Description\n",
        "* Cluster Silhouette\n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "* This notebook was written based on the guidelines provided in the walkthrough project 2: 'Churnometer'.\n",
        "* This notebook relates to the Data Understanding step of Crisp-DM methodology.\n",
        "* This notebook and the following will represent the learning outcome after following the Code Institute - Predictive Analytics and Machine Learning module.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd() \n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = (pd.read_csv(\"outputs/datasets/collection/breast-cancer.csv\")\n",
        "    .drop(['id', 'diagnosis'], axis=1)\n",
        "    )\n",
        "print(df.shape)\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cluster Pipeline with all data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ML Pipeline for Cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Our objective is to cluster similar data points and then analyse the clusters against the diagnostic (malignant or benign).\n",
        "\n",
        "* As a result, we will use only the thirty features (all variables but Diagnostic) to fit the cluster pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "### Feat Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "### PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "### ML algorithm\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def PipelineCluster():\n",
        "    pipeline_base = Pipeline([\n",
        "        \n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"PCA\", PCA(n_components=50, random_state=0)),\n",
        "        (\"model\", KMeans(n_clusters=50, random_state=0)),\n",
        "    ])\n",
        "\n",
        "    return pipeline_base\n",
        "\n",
        "PipelineCluster()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Principal Component Analysis (PCA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are interested to find the most suitable n_components, then we update the value in the ML Pipeline for Cluster\n",
        "\n",
        "* To reach that, we will create an object based on PipelineCluster(), then remove the last two steps (PCA and model): .steps[:-2]\n",
        "\n",
        "* Finally, the pipeline_pca scales the data, so we can apply PCA afterwards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_pca = Pipeline(pipeline_cluster.steps[:-2])\n",
        "df_pca = pipeline_pca.fit_transform(df)\n",
        "\n",
        "print(df_pca.shape,'\\n', type(df_pca))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Next, we apply PCA separately to the scaled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "# This line is used to display matplotlib plots inline within the Jupyter Notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "n_components = 16\n",
        "\n",
        "\n",
        "def pca_components_analysis(df_pca, n_components):\n",
        "    pca = PCA(n_components=n_components).fit(df_pca)\n",
        "    x_PCA = pca.transform(df_pca)  # array with transformed PCA\n",
        "\n",
        "    ComponentsList = [\"Component \" + str(number)\n",
        "                    for number in range(n_components)]\n",
        "    dfExplVarRatio = pd.DataFrame(\n",
        "        data=np.round(100 * pca.explained_variance_ratio_, 3),\n",
        "        index=ComponentsList,\n",
        "        columns=['Explained Variance Ratio (%)'])\n",
        "\n",
        "    dfExplVarRatio['Accumulated Variance'] = dfExplVarRatio['Explained Variance Ratio (%)'].cumsum(\n",
        "    )\n",
        "\n",
        "    PercentageOfDataExplained = dfExplVarRatio['Explained Variance Ratio (%)'].sum(\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"* The {n_components} components explain {round(PercentageOfDataExplained,2)}% of the data \\n\")\n",
        "    plt.figure(figsize=(9, 6))\n",
        "    sns.lineplot(data=dfExplVarRatio,  marker=\"o\")\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.yticks(np.arange(0, 110, 10))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "pca_components_analysis(df_pca=df_pca, n_components=n_components)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* With seven components we can achieve a bit more than 90% of data variance judging by the above figure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_components = 7\n",
        "\n",
        "pca = PCA(n_components=n_components).fit(df_pca)\n",
        "x_PCA = pca.transform(df_pca)\n",
        "\n",
        "ComponentsList = [\"Component \" + str(number) for number in range(n_components)]\n",
        "dfExplVarRatio = pd.DataFrame(\n",
        "    data= np.round(100 * pca.explained_variance_ratio_ ,3),\n",
        "    index=ComponentsList,\n",
        "    columns=['Explained Variance Ratio (%)'])\n",
        "\n",
        "PercentageOfDataExplained = dfExplVarRatio['Explained Variance Ratio (%)'].sum()\n",
        "\n",
        "print(f\"* The {n_components} components explain {round(PercentageOfDataExplained,2)}% of the data \\n\")\n",
        "print(dfExplVarRatio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We rewrite the PipelineCluster(), updating n_components to 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def PipelineCluster():\n",
        "    pipeline_base = Pipeline([\n",
        "\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"PCA\", PCA(n_components=7, random_state=0)),\n",
        "        (\"model\", KMeans(n_clusters=50, random_state=0)),\n",
        "    ])\n",
        "\n",
        "    return pipeline_base\n",
        "\n",
        "PipelineCluster()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Elbow Method and Silhouette Score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We will combine 2 techniques (Elbow Method and Silhouette Score) to find the optimal value for the number of clusters\n",
        "\n",
        "* We will transform the data up to the point that it will hit the model, for Elbow Method and Silhouette score.\n",
        "  * Therefore we remove the last step (.steps[:-1]) and fit_transform pipeline_analysis to the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_analysis = Pipeline(pipeline_cluster.steps[:-1])\n",
        "df_analysis = pipeline_analysis.fit_transform(df)\n",
        "\n",
        "print(df_analysis.shape,'\\n', type(df_analysis))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Elbow Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We use KElbowVisualizer() from YellowbrickElbow Analysis to implement the Elbow Method\n",
        "\n",
        "* We pass in as arguments the algorithm we want (KMeans) and the range for the number of clusters we want to try"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "\n",
        "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
        "plt.rcParams['font.sans-serif'] = ['DejaVu Sans']\n",
        "\n",
        "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(1,11)) # 11 is not inclusive\n",
        "visualizer.fit(df_analysis)\n",
        "visualizer.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the visualizer we can deduce -\n",
        "\n",
        "* The plot suggests 3 clusters\n",
        "* Between 2 and 5, the values have a sharp and steep falloff.|\n",
        "* Outside this range, it does not fall off in a similar manner."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Silhouette score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* By Silhouette score, we can interpret and validate the consistency within clusters, which is based on the mean intra-cluster distance and mean nearest-cluster distance for each data point.\n",
        "\n",
        "The silhouette score range is from -1 to +1.\n",
        "\n",
        "* “+1” means that a clustered data point is dense and properly separated from other clusters.\n",
        "* A score close to 0 means the clustered data point is overlapping with another cluster.\n",
        "* A negative score means that the clustered data point may be wrong; it may even belong to another cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "\n",
        "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
        "plt.rcParams['font.sans-serif'] = ['DejaVu Sans']\n",
        "print(\"=== Average Silhouette Score for different number of clusters ===\")\n",
        "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(2,5), metric='silhouette')\n",
        "visualizer.fit(df_analysis) \n",
        "visualizer.show() \n",
        "plt.show()\n",
        "print(\"\\n\")\n",
        "\n",
        "for n_clusters in np.arange(start=2,stop=11):\n",
        "\n",
        "    print(f\"=== Silhouette plot for {n_clusters} Clusters ===\")\n",
        "    visualizer = SilhouetteVisualizer(estimator = KMeans(n_clusters=n_clusters, random_state=0),\n",
        "                                        colors = 'yellowbrick')\n",
        "    visualizer.fit(df_analysis)\n",
        "    visualizer.show()\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Optimal Value for clusters -**\n",
        "\n",
        "* Elbow Method says 3.\n",
        "* The average Silhouette Score says 2, but the Silhouette Plot from 3 clusters is better than for 2 clusters.\n",
        "* As a result, we will pick 3, since the Elbow Method and Silhouette Plot both support that decision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* **We rewrite the PipelineCluster(), updating n_cluster to 3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def PipelineCluster():\n",
        "    pipeline_base = Pipeline([\n",
        "\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"PCA\", PCA(n_components=7, random_state=0)),\n",
        "        (\"model\", KMeans(n_clusters=3, random_state=0)),\n",
        "    ])\n",
        "\n",
        "    return pipeline_base\n",
        "\n",
        "PipelineCluster()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit Cluster Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Quick recap of our data for training cluster pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df.copy()\n",
        "print(X.shape)\n",
        "X.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Fit Cluster pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_cluster.fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Add cluster predictions to dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* **We add a column \"Clusters\" (with the Cluster Pipeline predictions) to X**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X['Clusters'] = pipeline_cluster['model'].labels_\n",
        "print(X.shape)\n",
        "X.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are interested to know the cluster frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"* Clusters frequencies \\n{ X['Clusters'].value_counts(normalize=True).to_frame().round(2)} \\n\\n\")\n",
        "X['Clusters'].value_counts().sort_values().plot(kind='bar')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*  There are three clusters, and the counting starts from 0.\n",
        "\n",
        "* We note that the algorithm found that the majority of the data (62%) belongs to cluster number 0.\n",
        "\n",
        "* Cluster 1 and 2 contain 21% and 17% of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=df_analysis[:, 0], y=df_analysis[:, 1],\n",
        "                hue=X['Clusters'], palette='Set1', alpha=0.6)\n",
        "plt.scatter(x=pipeline_cluster['model'].cluster_centers_[:, 0], y=pipeline_cluster['model'].cluster_centers_[:, 1],\n",
        "            marker=\"x\", s=169, linewidths=3, color=\"black\")\n",
        "plt.xlabel(\"PCA Component 0\")\n",
        "plt.ylabel(\"PCA Component 1\")\n",
        "plt.title(\"PCA Components colored by Clusters\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the scatterpolt above -\n",
        "\n",
        "* Cluster 0 has the highest density of data.\n",
        "* Overlap quantity among the data is very low which serves as a good point for cluster analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We save the cluster predictions from this pipeline to use in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cluster_predictions_with_all_variables = X['Clusters']\n",
        "cluster_predictions_with_all_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit a classifier, where the target is cluster predictions and features remaining variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Our new dataset has Clusters as a variable.\n",
        "* We use a technique where Clusters will be the target for a classifier, and the remaining variables will be features for that target."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We copy X to a DataFrame df_clf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_clf = X.copy()\n",
        "print(df_clf.shape)\n",
        "df_clf.tail(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Split Train and Test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df_clf.drop(['Clusters'], axis=1),\n",
        "    df_clf['Clusters'],\n",
        "    test_size=0.2,\n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Create classifier pipeline steps.\n",
        "* We are considering a model that typically offers good results, and feature's importance can be assessed with .features_importance_ using a tree-based algorithm. \n",
        "* We are using AdaBoostClassifier since it had good performance in the previous notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feat Selection\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# ML algorithm\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "def PipelineClf2ExplainClusters():\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"feat_selection\", SelectFromModel(GradientBoostingClassifier(random_state=0)) ),\n",
        "        (\"model\", GradientBoostingClassifier(random_state=0)),\n",
        "    ])\n",
        "\n",
        "    return pipeline_base\n",
        "\n",
        "PipelineClf2ExplainClusters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Fit the classifier to the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_clf_cluster = PipelineClf2ExplainClusters()\n",
        "pipeline_clf_cluster.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate classifier performance on Train and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_train, pipeline_clf_cluster.predict(X_train)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(classification_report(y_test, pipeline_clf_cluster.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assess the most important Features that define a cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_clf_cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Taking pipeline steps upto classifier without including it\n",
        "pipeline_feat_select = Pipeline(pipeline_clf_cluster.steps[:-2])\n",
        "\n",
        "# Fit-transform the feature selection pipeline\n",
        "X_train_selected = pipeline_feat_select.fit_transform(X_train, y_train)\n",
        "\n",
        "# The columns that survive the selection\n",
        "selected_columns = X_train.columns[pipeline_clf_cluster['feat_selection'].get_support()]\n",
        "\n",
        "# create DataFrame to display feature importance\n",
        "df_feature_importance = (pd.DataFrame({\n",
        "    'Feature': selected_columns,\n",
        "    'Importance': pipeline_clf_cluster['model'].feature_importances_})\n",
        "                         .sort_values(by='Importance', ascending=False)\n",
        ")\n",
        "\n",
        "# assign best features in importance order\n",
        "best_features = df_feature_importance['Feature'].to_list()\n",
        "\n",
        "# Most important features statement and plot\n",
        "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
        "    f\"The model was trained on them: \\n{best_features} \\n\")\n",
        "df_feature_importance.plot(kind='bar', x='Feature', y='Importance')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These are the 4 most important features in descending order -\n",
        "\n",
        "* ['concavity_mean', 'area_worst', 'fractal_dimension_worst', 'perimeter_worst']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_features_pipeline_all_variables = best_features\n",
        "best_features_pipeline_all_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cluster Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We load function that plots a table with description for all Clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def DescriptionAllClusters(df, decimal_points=3):\n",
        "\n",
        "  DescriptionAllClusters = pd.DataFrame(columns=df.drop(['Clusters'],axis=1).columns)\n",
        "  # iterate on each cluster , calls Clusters_IndividualDescription()\n",
        "  for cluster in df.sort_values(by='Clusters')['Clusters'].unique():\n",
        "    \n",
        "      EDA_ClusterSubset = df.query(f\"Clusters == {cluster}\").drop(['Clusters'],axis=1)\n",
        "      ClusterDescription = Clusters_IndividualDescription(EDA_ClusterSubset,cluster,decimal_points)\n",
        "      DescriptionAllClusters = pd.concat([DescriptionAllClusters, ClusterDescription], ignore_index=True)\n",
        "\n",
        "  \n",
        "  DescriptionAllClusters.set_index(['Cluster'],inplace=True)\n",
        "  return DescriptionAllClusters\n",
        "\n",
        "\n",
        "def Clusters_IndividualDescription(EDA_Cluster,cluster, decimal_points):\n",
        "\n",
        "  ClustersDescription = pd.DataFrame(columns=EDA_Cluster.columns)\n",
        "  # for a given cluster, iterate in all columns\n",
        "  # if the variable is numerical, calculate the IQR: display as Q1 -- Q3.\n",
        "    # That will show the range for the most common values for the numerical variable\n",
        "  # if the variable is categorical, count the frequencies and display the top 3 most frequent\n",
        "    # That will show the most common levels for the category\n",
        "\n",
        "  for col in EDA_Cluster.columns:\n",
        "    \n",
        "    try:  # eventually a given cluster will have only missing data for a given variable\n",
        "      \n",
        "      if EDA_Cluster[col].dtypes == 'object':\n",
        "        \n",
        "        top_frequencies = EDA_Cluster.dropna(subset=[col])[[col]].value_counts(normalize=True).nlargest(n=3)\n",
        "        Description = ''\n",
        "        \n",
        "        for x in range(len(top_frequencies)):\n",
        "          freq = top_frequencies.iloc[x]\n",
        "          category = top_frequencies.index[x][0]\n",
        "          CategoryPercentage = int(round(freq*100,0))\n",
        "          statement =  f\"'{category}': {CategoryPercentage}% , \"  \n",
        "          Description = Description + statement\n",
        "        \n",
        "        ClustersDescription.at[0,col] = Description[:-2]\n",
        "\n",
        "\n",
        "      \n",
        "      elif EDA_Cluster[col].dtypes in ['float', 'int']:\n",
        "        DescStats = EDA_Cluster.dropna(subset=[col])[[col]].describe()\n",
        "        Q1 = round(DescStats.iloc[4,0], decimal_points)\n",
        "        Q3 = round(DescStats.iloc[6,0], decimal_points)\n",
        "        Description = f\"{Q1} -- {Q3}\"\n",
        "        ClustersDescription.at[0,col] = Description\n",
        "    \n",
        "    \n",
        "    except Exception as e:\n",
        "      ClustersDescription.at[0,col] = 'Not available'\n",
        "      print(f\"** Error Exception: {e} - cluster {cluster}, variable {col}\")\n",
        "  \n",
        "  ClustersDescription['Cluster'] = str(cluster)\n",
        "  \n",
        "  return ClustersDescription\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We load a custom function to plot cluster distribution per Variable (absolute and relative levels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "\n",
        "def cluster_distribution_per_variable(df, target):\n",
        "    \"\"\"\n",
        "    The data should have 2 variables, the cluster predictions and\n",
        "    the variable you want to analyze with, in this case we call \"target\".\n",
        "    We use plotly express to create 2 plots:\n",
        "    Cluster distribution across the target.\n",
        "    Relative presence of the target level in each cluster.\n",
        "    \"\"\"\n",
        "    df_bar_plot = df.groupby(['Clusters', target]).size().reset_index(name='Count')\n",
        "    df_bar_plot.columns = ['Clusters', target, 'Count']\n",
        "    df_bar_plot[target] = df_bar_plot[target].astype('object')\n",
        "\n",
        "    print(f\"Clusters distribution across {target} levels\")\n",
        "    fig = px.bar(df_bar_plot, x='Clusters', y='Count',\n",
        "                 color=target, width=800, height=500)\n",
        "    fig.update_layout(xaxis=dict(tickmode='array',\n",
        "                      tickvals=df['Clusters'].unique()))\n",
        "    fig.show(renderer='jupyterlab')\n",
        "\n",
        "    df_relative = (df\n",
        "                   .groupby([\"Clusters\", target])\n",
        "                   .size()\n",
        "                   .unstack(fill_value=0)\n",
        "                   .apply(lambda x: 100 * x / x.sum(), axis=1)\n",
        "                   .stack()\n",
        "                   .reset_index(name='Relative Percentage (%)')\n",
        "                   .sort_values(by=['Clusters', target])\n",
        "                   )\n",
        "\n",
        "    print(f\"Relative Percentage (%) of {target} in each cluster\")\n",
        "    fig = px.line(df_relative, x='Clusters', y='Relative Percentage (%)',\n",
        "                  color=target, width=800, height=500)\n",
        "    fig.update_layout(xaxis=dict(tickmode='array',\n",
        "                      tickvals=df['Clusters'].unique()))\n",
        "    fig.update_traces(mode='markers+lines')\n",
        "    fig.show(renderer='jupyterlab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We create a DataFrame that contains best features and Clusters Predictions since we want to analyse the patterns for each cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cluster_profile = df_clf.copy()\n",
        "df_cluster_profile = df_cluster_profile.filter(items=best_features + ['Clusters'], axis=1)\n",
        "print(df_cluster_profile.shape)\n",
        "df_cluster_profile.tail(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We want to analyse diagnosis levels also, so we load its data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_diagnosis = pd.read_csv(\"outputs/datasets/collection/breast-cancer.csv\").filter(['diagnosis'])\n",
        "df_diagnosis['diagnosis'] = df_diagnosis['diagnosis'].astype('object')\n",
        "df_diagnosis.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cluster profile based on the best features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "clusters_profile = df=pd.concat([df_cluster_profile,df_diagnosis], axis=1)\n",
        "clusters_profile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "clusters_profile = DescriptionAllClusters(df=pd.concat([df_cluster_profile,df_diagnosis], axis=1),\n",
        "                                          decimal_points=3)\n",
        "clusters_profile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cluster analysis from each profile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since we have set '0' as Benign (B) and '1' as Malignant (M), from the above profiling we can describe each clusters in the following -\n",
        "\n",
        "* **Cluster 0 Profile**\n",
        "\n",
        "    - In Cluster 0, mean concavity values range from 0.02 to 0.057, indicating smooth tumor contours. The worst area measurements, ranging from 468.9 to 706.1, suggest smaller tumor sizes, while the worst fractal dimension falls between 0.069 and 0.082, indicating low structural chaos. Perimeter values of 79.79 to 99.08 indicate compact shapes. Notably, 91% of diagnoses in this cluster are benign, with only 9% malignant, aligning with the low-risk features.\n",
        "\n",
        "* **Cluster 1 Profile**\n",
        "\n",
        "    - Cluster 1 shows mean concavity from 0.127 to 0.22, indicating irregular tumor margins. The worst area is significantly larger, ranging from 1437.0 to 2009.0, while the worst fractal dimension (0.076 to 0.092) suggests moderate disorder. The worst perimeter ranges from 143.7 to 171.1, reflecting aggressive tumor growth. Notably, 100% of the diagnoses in this cluster are malignant (M), consistent with a high-risk profile.\n",
        "\n",
        "- **Cluster 2 Profile**\n",
        "\n",
        "    - In Cluster 2, mean concavity ranges from 0.104 to 0.186, indicating moderately irregular shapes. The worst area spans from 580.6 to 975.2, and the worst perimeter varies between 94.22 and 122.1, suggesting mid-range tumor size. The worst fractal dimension, from 0.096 to 0.12, is the highest among all clusters, showing chaotic tumor structures. Diagnoses are mixed, with 64% malignant (M) and 36% benign (B), indicating possible early-stage malignancies or borderline cases needing further examination."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clusters distribution across diagnosis levels & Relative Percentage of diagnosis in each cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cluster_vs_diagnosis = df_diagnosis.copy()\n",
        "df_cluster_vs_diagnosis['Clusters'] = X['Clusters']\n",
        "cluster_distribution_per_variable(df=df_cluster_vs_diagnosis, target='diagnosis')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit New Cluster Pipeline with most important features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* In order to reduce feature space, we will study the trade-off between the previous Cluster Pipeline (fitted with all variables) and Pipeline using the variables that are most important to define the clusters from the previous pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_features_pipeline_all_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define trade-off and metrics to compare new and previous Cluster Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To evaluate this trade-off we will -\n",
        "\n",
        "1. Conduct a elbow method and silhouette analysis and check if the same number of clusters is suggested\n",
        "2. Fit new cluster pipeline and compare if the predictions from this pipeline are \"equivalent\" to the predictions from the previous pipeline\n",
        "3. Fit a classifier to explain cluster, and check if performance on Train and Test sets is similar to the previous pipeline\n",
        "4. Check if the most important features for the classifier are the same from the previous pipeline\n",
        "5. Compare if the cluster profile from both pipelines are \"equivalent\"\n",
        "\n",
        "* Should we express our willingness to support them, we may implement a cluster pipeline that utilizes the features most indicative of the clusters identified in the previous pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Subset data with the most relevant variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_reduced = df.filter(best_features_pipeline_all_variables)\n",
        "print(df_reduced.shape)\n",
        "df_reduced.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Rewrite Cluster Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def PipelineCluster():\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"model\", KMeans(n_clusters=3, random_state=0)),\n",
        "    ])\n",
        "\n",
        "    return pipeline_base\n",
        "\n",
        "PipelineCluster()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Apply Elbow Method and Silhouette analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_analysis = Pipeline(pipeline_cluster.steps[:-1])\n",
        "df_analysis = pipeline_analysis.fit_transform(df_reduced)\n",
        "\n",
        "print(df_analysis.shape,'\\n', type(df_analysis))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Elbow Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "print(\"=== Average Elbow Score for different number of clusters ===\")\n",
        "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(1,11))\n",
        "visualizer.fit(df_analysis) \n",
        "visualizer.show() \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the visualizer we can deduce -\n",
        "\n",
        "* The plot suggests 3 clusters\n",
        "* Between 2 and 5, the values have a sharp and steep falloff.|\n",
        "* Outside this range, it does not fall off in a similar manner."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Silhoutte Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "\n",
        "n_cluster_start, n_cluster_stop = 2, 5\n",
        "\n",
        "print(\"=== Average Silhouette Score for different number of clusters ===\")\n",
        "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(\n",
        "    n_cluster_start, n_cluster_stop), metric='silhouette')\n",
        "visualizer.fit(df_analysis)\n",
        "visualizer.show()\n",
        "plt.show()\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "for n_clusters in np.arange(start=n_cluster_start, stop=n_cluster_stop):\n",
        "\n",
        "    print(f\"=== Silhouette plot for {n_clusters} Clusters ===\")\n",
        "    visualizer = SilhouetteVisualizer(estimator=KMeans(n_clusters=n_clusters, random_state=0),\n",
        "                                      colors='yellowbrick')\n",
        "    visualizer.fit(df_analysis)\n",
        "    visualizer.show()\n",
        "    plt.show()\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Optimal Value for clusters with best features-**\n",
        "\n",
        "* Elbow Method says 3.\n",
        "* The average Silhouette Score says 2, but the Silhouette Plot from 3 clusters is better than for 2 clusters.\n",
        "* As a result, we will pick 3, since the Elbow Method and Silhouette Plot both support that decision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit New Cluster Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We set X as our training set for the cluster. It is a copy of df_reduced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df_reduced.copy()\n",
        "print(X.shape)\n",
        "X.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fit Cluster pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_cluster.fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Add cluster predictions to dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We add a column \"Clusters\" (with the cluster pipeline predictions) to the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X['Clusters'] = pipeline_cluster['model'].labels_\n",
        "print(X.shape)\n",
        "X.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"* Clusters frequencies \\n{ X['Clusters'].value_counts(normalize=True).to_frame().round(2)} \\n\\n\")\n",
        "X['Clusters'].value_counts().sort_values().plot(kind='bar')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare current cluster predictions to previous cluster predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We just fitted a new cluster pipeline and want to compare if its predictions are \"equivalent\" to the previous cluster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* These are the predictions from the previous cluster pipeline - trained with all variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cluster_predictions_with_all_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* And these are the predictions from current cluster pipeline, trained with ['concavity_mean', 'area_worst', 'fractal_dimension_worst', 'perimeter_worst']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cluster_predictions_with_best_features = X['Clusters'] \n",
        "cluster_predictions_with_best_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### We use a confusion matrix to evaluate if the predictions of both pipelines are \"equivalent\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(cluster_predictions_with_all_variables, cluster_predictions_with_best_features))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We can see that the cluster alignment is matched perfectly between the two and the data spread is similar, other than cluster 2 being heavier on the Malignant side."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Fit a classifier, where the target is cluster predictions and features remaining variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_clf = X.copy()\n",
        "print(df_clf.shape)\n",
        "df_clf.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Split Train and Test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df_clf.drop(['Clusters'], axis=1),\n",
        "    df_clf['Clusters'],\n",
        "    test_size=0.2,\n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "print(X_train.shape, X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Rewrite pipeline to explain clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def PipelineClf2ExplainClusters():\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        # we don't consider feature selection step, since we know which features to consider\n",
        "        (\"model\", GradientBoostingClassifier(random_state=0)),\n",
        "\n",
        "    ])\n",
        "    return pipeline_base\n",
        "\n",
        "\n",
        "PipelineClf2ExplainClusters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fit a classifier, where the target is cluster labels and features remaining variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We create and fit a classifier pipeline to learn the feature importance when defining a cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_clf_cluster = PipelineClf2ExplainClusters()\n",
        "pipeline_clf_cluster.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate classifier performance on Train and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(classification_report(y_train, pipeline_clf_cluster.predict(X_train)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "print(classification_report(y_test, pipeline_clf_cluster.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assess Most Important Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# since we don't have feature selection step in this pipeline, best_features is Xtrain columns\n",
        "best_features = X_train.columns.to_list()\n",
        "\n",
        "# create a DataFrame to display feature importance\n",
        "df_feature_importance = (pd.DataFrame(data={\n",
        "    'Feature': best_features,\n",
        "    'Importance': pipeline_clf_cluster['model'].feature_importances_})\n",
        "    .sort_values(by='Importance', ascending=False)\n",
        ")\n",
        "\n",
        "best_features = df_feature_importance['Feature'].to_list()\n",
        "\n",
        "# Most important features statement and plot\n",
        "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
        "      f\"The model was trained on them: \\n{df_feature_importance['Feature'].to_list()}\")\n",
        "\n",
        "df_feature_importance.plot(kind='bar', x='Feature', y='Importance')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cluster Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We create a DataFrame that contains the best features and Clusters Predictions: we want to analyse the patterns for each cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cluster_profile = df_clf.copy()\n",
        "df_cluster_profile = df_cluster_profile.filter(items=best_features + ['Clusters'], axis=1)\n",
        "df_cluster_profile.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We analyse diagnosis levels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_diagnosis = pd.read_csv(\"outputs/datasets/collection/breast-cancer.csv\").filter(['diagnosis'])\n",
        "df_diagnosis['diagnosis'] = df_diagnosis['diagnosis'].astype('object')\n",
        "df_diagnosis.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cluster profile on most important features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "clusters_profile = DescriptionAllClusters(df= pd.concat([df_cluster_profile,df_diagnosis], axis=1))\n",
        "clusters_profile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clusters distribution across diagnosis levels & Relative Percentage of diagnosis in each cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cluster_vs_diagnosis = df_diagnosis.copy()\n",
        "df_cluster_vs_diagnosis['Clusters'] = X['Clusters']\n",
        "cluster_distribution_per_variable(df=df_cluster_vs_diagnosis, target='diagnosis')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparative analysis of the cluster profiles between the original pipeline and the new pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here’s a concise comparative analysis of the cluster profiles between the original pipeline and the new pipeline:\n",
        "\n",
        "* **Cluster 0 Comparison**\n",
        "\n",
        "    - The new data retains Cluster 0's low-risk characteristics, with slightly broader ranges for all features. The range for `perimeter_worst` is now 79.848 to 99.285 (vs. 79.79 to 99.08), while `concavity_mean` is 0.021 to 0.06 (vs. 0.02 to 0.057). The proportion of benign diagnoses dropped from 91% to 90%, with malignant cases rising from 9% to 10%. The consistency in `fractal_dimension_worst` (0.069 to 0.083 vs. 0.069 to 0.082) confirms reliable identification of benign tumors despite using fewer features.\n",
        "\n",
        "* **Cluster 1 Comparison**\n",
        "\n",
        "    - Cluster 1 continues to show 100% malignancy in both datasets. The ranges for `concavity_mean` (0.133 to 0.22 vs. 0.127 to 0.22) and `fractal_dimension_worst` (0.076 to 0.093 vs. 0.076 to 0.092) remain nearly identical. The boundaries for `perimeter_worst` (145.2 to 171.325) and `area_worst` (1436.75 to 2009.25) are also unchanged, indicating these features effectively capture this high-risk profile.\n",
        "\n",
        "* **Cluster 2 Comparison**\n",
        "\n",
        "    - The new model shows increased malignancy detection in Cluster 2 (72% vs. 64%), suggesting better capture of malignant characteristics. All ranges slightly expanded: `perimeter_worst` (99.175 to 122.95 vs. 94.22 to 122.1), `concavity_mean` (0.114 to 0.201 vs. 0.104 to 0.186), and `area_worst` (639.2 to 981.05 vs. 580.6 to 975.2). Notably, `fractal_dimension_worst` increased to 0.103 to 0.124, reinforcing the link between high fractal dimension and malignancy. The benign proportion dropped from 36% to 28%, indicating fewer false negatives.\n",
        "\n",
        "Overall, the four-feature model aligns well with the original 30-feature version, particularly for clear benign or malignant cases. Cluster 2's increased malignancy percentage suggests these features are effective in identifying high-risk tumors, while the broader ranges indicate slightly less precision in borderline cases. The consistency across Cluster 0 and Cluster 1 further confirms the reliability of the 4 feature-cluster pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Conclusion and Pushing files to Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Which pipeline should I deploy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We recap the criteria we consider to evaluate the trade-off -\n",
        "\n",
        "* *Conduct an elbow method and silhouette analysis and check if the same number of clusters is suggested.*\n",
        "* *Fit a new cluster pipeline and compare if the predictions from this pipeline are \"equivalent\" to the predictions from the previous pipeline.*\n",
        "* *Fit a classifier to explain cluster and check if performance on Train and Test sets is similar to the previous pipeline.*\n",
        "* *Check if the most important features for the classifier are the same from the previous pipeline.*\n",
        "* *Compare if the cluster profile from both pipelines is \"equivalent\".*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**To conclude** -\n",
        "\n",
        "* Following a comprehensive comparative analysis of the existing and newly developed pipelines, I have determined that the newer pipeline, which incorporates only four features identified as optimal for cluster analysis, is more advantageous.\n",
        "\n",
        "* This model retains diagnostic accuracy while enhancing computational efficiency.\n",
        "\n",
        "* The streamlined approach ensures a clear distinction between benign and malignant clusters (indicated as 0 and 1), and it improves the detection of malignancies in borderline cases (designated as Cluster 2).\n",
        "\n",
        "* Furthermore, the dimensionality reduction enhances its scalability for clinical implementation without compromising essential diagnostic performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_cluster # selected pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "## Push files to Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will generate the following files\n",
        "\n",
        "* Cluster Pipeline\n",
        "* Train Set\n",
        "* Feature importance plot\n",
        "* Clusters Description\n",
        "* Cluster Silhouette"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "version = 'v1'\n",
        "file_path = f'outputs/ml_pipeline/cluster_analysis/{version}'\n",
        "\n",
        "try:\n",
        "    os.makedirs(name=file_path)\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cluster pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "joblib.dump(value=pipeline_cluster, filename=f\"{file_path}/cluster_pipeline.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df_reduced.shape)\n",
        "df_reduced.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_reduced.to_csv(f\"{file_path}/TrainSet.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Most important features plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* These are the features that define a cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_feature_importance.plot(kind='bar', x='Feature', y='Importance', figsize=(8,4))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_feature_importance.plot(kind='bar', x='Feature', y='Importance', figsize=(8,4))\n",
        "plt.savefig(f\"{file_path}/features_define_cluster.png\", bbox_inches='tight', dpi=150)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cluster Profile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clusters_profile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clusters_profile.to_csv(f\"{file_path}/clusters_profile.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cluster silhouette plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "visualizer = SilhouetteVisualizer(Pipeline(pipeline_cluster.steps[-1:])[0], colors='yellowbrick')\n",
        "visualizer.fit(df_analysis)\n",
        "visualizer.show()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(figsize=(7,5))\n",
        "fig = SilhouetteVisualizer(Pipeline(pipeline_cluster.steps[-1:])[0] , colors='yellowbrick', ax=axes)\n",
        "fig.fit(df_analysis)\n",
        "\n",
        "plt.savefig(f\"{file_path}/clusters_silhouette.png\", bbox_inches='tight',dpi=150)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
