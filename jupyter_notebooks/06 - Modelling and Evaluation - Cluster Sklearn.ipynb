{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Cluster**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Fit and evaluate a cluster model to group similar data\n",
        "* Analyse the clusters against the diagnostic (malignant or benign)\n",
        "* Understand the profile for each cluster\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* Outputs/datasets/collection/breast-cancer.csv\n",
        "* Instructions on which variables to use for data cleaning and feature engineering, which are found in their respective notebooks.\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Cluster Pipeline\n",
        "* Train Set\n",
        "* Most important features to define a cluster plot\n",
        "* Clusters Profile Description\n",
        "* Cluster Silhouette\n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "* This notebook was written based on the guidelines provided in the walkthrough project 2: 'Churnometer'.\n",
        "* This notebook relates to the Data Understanding step of Crisp-DM methodology.\n",
        "* This notebook and the following will represent the learning outcome after following the Code Institute - Predictive Analytics and Machine Learning module.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = (pd.read_csv(\"outputs/datasets/collection/breast-cancer.csv\")\n",
        "    .drop(['id', 'diagnosis'], axis=1)\n",
        "    )\n",
        "print(df.shape)\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cluster Pipeline with all data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ML Pipeline for Cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Our objective is to cluster similar data points and then analyse the clusters against the diagnostic (malignant or benign).\n",
        "\n",
        "* As a result, we will use only the thirty features (all variables but Diagnostic) to fit the cluster pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "### Feat Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "### PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "### ML algorithm\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def PipelineCluster():\n",
        "    pipeline_base = Pipeline([\n",
        "\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"PCA\", PCA(n_components=50, random_state=0)),\n",
        "        (\"model\", KMeans(n_clusters=50, random_state=0)),\n",
        "    ])\n",
        "\n",
        "    return pipeline_base\n",
        "\n",
        "PipelineCluster()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Principal Component Analysis (PCA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are interested to find the most suitable n_components, then we update the value in the ML Pipeline for Cluster\n",
        "\n",
        "* To reach that, we will create an object based on PipelineCluster(), then remove the last two steps (PCA and model): .steps[:-2]\n",
        "\n",
        "* Finally, the pipeline_pca scales the data, so we can apply PCA afterwards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_pca = Pipeline(pipeline_cluster.steps[:-2])\n",
        "df_pca = pipeline_pca.fit_transform(df)\n",
        "\n",
        "print(df_pca.shape,'\\n', type(df_pca))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Next, we apply PCA separately to the scaled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "# This line is used to display matplotlib plots inline within the Jupyter Notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "n_components = 30\n",
        "\n",
        "\n",
        "def pca_components_analysis(df_pca, n_components):\n",
        "    pca = PCA(n_components=n_components).fit(df_pca)\n",
        "    x_PCA = pca.transform(df_pca)  # array with transformed PCA\n",
        "\n",
        "    ComponentsList = [\"Component \" + str(number)\n",
        "                    for number in range(n_components)]\n",
        "    dfExplVarRatio = pd.DataFrame(\n",
        "        data=np.round(100 * pca.explained_variance_ratio_, 3),\n",
        "        index=ComponentsList,\n",
        "        columns=['Explained Variance Ratio (%)'])\n",
        "\n",
        "    dfExplVarRatio['Accumulated Variance'] = dfExplVarRatio['Explained Variance Ratio (%)'].cumsum(\n",
        "    )\n",
        "\n",
        "    PercentageOfDataExplained = dfExplVarRatio['Explained Variance Ratio (%)'].sum(\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"* The {n_components} components explain {round(PercentageOfDataExplained,2)}% of the data \\n\")\n",
        "    plt.figure(figsize=(9, 6))\n",
        "    sns.lineplot(data=dfExplVarRatio,  marker=\"o\")\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.yticks(np.arange(0, 110, 10))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "pca_components_analysis(df_pca=df_pca, n_components=n_components)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* With seven components we can achieve a bit more than 90% of data variance judging by the above figure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_components = 7\n",
        "\n",
        "pca = PCA(n_components=n_components).fit(df_pca)\n",
        "x_PCA = pca.transform(df_pca)\n",
        "\n",
        "ComponentsList = [\"Component \" + str(number) for number in range(n_components)]\n",
        "dfExplVarRatio = pd.DataFrame(\n",
        "    data= np.round(100 * pca.explained_variance_ratio_ ,3),\n",
        "    index=ComponentsList,\n",
        "    columns=['Explained Variance Ratio (%)'])\n",
        "\n",
        "PercentageOfDataExplained = dfExplVarRatio['Explained Variance Ratio (%)'].sum()\n",
        "\n",
        "print(f\"* The {n_components} components explain {round(PercentageOfDataExplained,2)}% of the data \\n\")\n",
        "print(dfExplVarRatio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We rewrite the PipelineCluster(), updating n_components to 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def PipelineCluster():\n",
        "    pipeline_base = Pipeline([\n",
        "\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"PCA\", PCA(n_components=7, random_state=0)),\n",
        "        (\"model\", KMeans(n_clusters=50, random_state=0)),\n",
        "    ])\n",
        "\n",
        "    return pipeline_base\n",
        "\n",
        "PipelineCluster()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Elbow Method and Silhouette Score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We will combine 2 techniques (Elbow Method and Silhouette Score) to find the optimal value for the number of clusters\n",
        "\n",
        "* We will transform the data up to the point that it will hit the model, for Elbow Method and Silhouette score.\n",
        "  * Therefore we remove the last step (.steps[:-1]) and fit_transform pipeline_analysis to the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_analysis = Pipeline(pipeline_cluster.steps[:-1])\n",
        "df_analysis = pipeline_analysis.fit_transform(df)\n",
        "\n",
        "print(df_analysis.shape,'\\n', type(df_analysis))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Elbow Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We use KElbowVisualizer() from YellowbrickElbow Analysis to implement the Elbow Method\n",
        "\n",
        "* We pass in as arguments the algorithm we want (KMeans) and the range for the number of clusters we want to try"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\"findfont.*\") # Ignore font warnings\n",
        "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(1,11)) # 11 is not inclusive\n",
        "visualizer.fit(df_analysis)\n",
        "visualizer.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the visualizer we can deduce -\n",
        "\n",
        "* The plot suggests three clusters\n",
        "* Between 2 and 5, the values have a sharp and steep falloff.\n",
        "* Outside this range, it does not fall off in a similar manner."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Silhouette score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* By Silhouette score, we can interpret and validate the consistency within clusters, which is based on the mean intra-cluster distance and mean nearest-cluster distance for each data point.\n",
        "\n",
        "The silhouette score range is from -1 to +1.\n",
        "\n",
        "* “+1” means that a clustered data point is dense and properly separated from other clusters.\n",
        "* A score close to 0 means the clustered data point is overlapping with another cluster.\n",
        "* A negative score means that the clustered data point may be wrong; it may even belong to another cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "\n",
        "print(\"=== Average Silhouette Score for different number of clusters ===\")\n",
        "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(2,7), metric='silhouette')\n",
        "visualizer.fit(df_analysis) \n",
        "visualizer.show() \n",
        "plt.show()\n",
        "print(\"\\n\")\n",
        "\n",
        "for n_clusters in np.arange(start=2,stop=11):\n",
        "\n",
        "    print(f\"=== Silhouette plot for {n_clusters} Clusters ===\")\n",
        "    visualizer = SilhouetteVisualizer(estimator = KMeans(n_clusters=n_clusters, random_state=0),\n",
        "                                        colors = 'yellowbrick')\n",
        "    visualizer.fit(df_analysis)\n",
        "    visualizer.show()\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Optimal Value for clusters -**\n",
        "\n",
        "* Elbow Method says three.\n",
        "* The average Silhouette Score says two, but the Silhouette Plot from three clusters is better than for two clusters.\n",
        "* As a result, we will pick three, since the Elbow Method and Silhouette Plot both support that decision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* **We rewrite the PipelineCluster(), updating n_cluster to 3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def PipelineCluster():\n",
        "    pipeline_base = Pipeline([\n",
        "\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"PCA\", PCA(n_components=7, random_state=0)),\n",
        "        (\"model\", KMeans(n_clusters=3, random_state=0)),\n",
        "    ])\n",
        "\n",
        "    return pipeline_base\n",
        "\n",
        "PipelineCluster()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit Cluster Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Quick recap of our data for training cluster pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df.copy()\n",
        "print(X.shape)\n",
        "X.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Fit Cluster pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_cluster.fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Add cluster predictions to dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* **We add a column \"Clusters\" (with the Cluster Pipeline predictions) to X**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X['Clusters'] = pipeline_cluster['model'].labels_\n",
        "print(X.shape)\n",
        "X.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are interested to know the cluster frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"* Clusters frequencies \\n{ X['Clusters'].value_counts(normalize=True).to_frame().round(2)} \\n\\n\")\n",
        "X['Clusters'].value_counts().sort_values().plot(kind='bar')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Load and Inspect Kaggle Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Convert 'diagnosis' values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We will convert `diagnosis` values from `M` and `B` to `1` and `0` respectively so that the saved dataset will already have the target variable in a numeric format, so any ML model can consume it directly without extra preprocessing later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Push files to Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* In case you don't need to push files to Repo, you may replace this section with \"Conclusions and Next Steps\" and state your conclusions and next steps."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "3.12.1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
